{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b184e83a-770d-4cc1-b9ae-09e5fee774d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¢ Training Stacking Model 0...\n",
      "\n",
      "Fitting LGBM-RF\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Info] Number of positive: 184203, number of negative: 672981\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15703\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214893 -> initscore=-1.295679\n",
      "[LightGBM] [Info] Start training from score -1.295679\n",
      "Fitting LGBM\n",
      "[LightGBM] [Info] Number of positive: 184203, number of negative: 672981\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 15703\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 64 dense feature groups (52.32 MB) transferred to GPU in 0.036338 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214893 -> initscore=-1.295679\n",
      "[LightGBM] [Info] Start training from score -1.295679\n",
      "Fitting XGBoost\n",
      "Fitting MLP\n",
      "âœ… Models refitted on the new dataset\n",
      "ðŸš€ Training Stacking Model\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [02:56:42] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Accuracy: 0.8842\n",
      "âœ… Recall: 0.6540\n",
      "âœ… F1 Score: 0.7093\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Best Threshold: 0.950\n",
      "âœ… Best Weighted Signal Significance (S/âˆš(S+B)): 7.8194\n",
      "âœ… Total Weighted Surviving Background Events: 71.8304\n",
      "âœ… Weighted False Positives Per Background Type:\n",
      "  HH: 0.0000\n",
      "  ZZ: 0.9868\n",
      "  ZH: 0.2516\n",
      "  WW: 0.0000\n",
      "  tt: 10.0600\n",
      "  qqX: 0.0000\n",
      "  qqqqX: 0.0000\n",
      "  qqHX: 0.5950\n",
      "  qq: 5.4095\n",
      "  pebb: 22.6080\n",
      "  pebbqq: 4.5660\n",
      "  peqqH: 25.3585\n",
      "  pett: 1.9950\n",
      "\n",
      "ðŸŸ¢ Training Stacking Model 1...\n",
      "\n",
      "Fitting LGBM-RF\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Info] Number of positive: 184359, number of negative: 672825\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.141924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15705\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.215075 -> initscore=-1.294600\n",
      "[LightGBM] [Info] Start training from score -1.294600\n",
      "Fitting LGBM\n",
      "[LightGBM] [Info] Number of positive: 184359, number of negative: 672825\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 15705\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 64 dense feature groups (52.32 MB) transferred to GPU in 0.023358 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.215075 -> initscore=-1.294600\n",
      "[LightGBM] [Info] Start training from score -1.294600\n",
      "Fitting XGBoost\n",
      "Fitting MLP\n",
      "âœ… Models refitted on the new dataset\n",
      "ðŸš€ Training Stacking Model\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [03:01:09] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Accuracy: 0.8839\n",
      "âœ… Recall: 0.6554\n",
      "âœ… F1 Score: 0.7085\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Best Threshold: 0.932\n",
      "âœ… Best Weighted Signal Significance (S/âˆš(S+B)): 8.1484\n",
      "âœ… Total Weighted Surviving Background Events: 85.5965\n",
      "âœ… Weighted False Positives Per Background Type:\n",
      "  HH: 0.0000\n",
      "  ZZ: 0.9868\n",
      "  ZH: 0.4313\n",
      "  WW: 2.5745\n",
      "  tt: 17.6050\n",
      "  qqX: 0.2174\n",
      "  qqqqX: 0.4000\n",
      "  qqHX: 0.7200\n",
      "  qq: 7.6780\n",
      "  pebb: 15.0720\n",
      "  pebbqq: 7.6100\n",
      "  peqqH: 30.3065\n",
      "  pett: 1.9950\n",
      "\n",
      "ðŸŸ¢ Training Stacking Model 2...\n",
      "\n",
      "Fitting LGBM-RF\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Info] Number of positive: 184287, number of negative: 672897\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.154338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15707\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214991 -> initscore=-1.295098\n",
      "[LightGBM] [Info] Start training from score -1.295098\n",
      "Fitting LGBM\n",
      "[LightGBM] [Info] Number of positive: 184287, number of negative: 672897\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 15707\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 64 dense feature groups (52.32 MB) transferred to GPU in 0.024140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214991 -> initscore=-1.295098\n",
      "[LightGBM] [Info] Start training from score -1.295098\n",
      "Fitting XGBoost\n",
      "Fitting MLP\n",
      "âœ… Models refitted on the new dataset\n",
      "ðŸš€ Training Stacking Model\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [03:05:33] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Accuracy: 0.8847\n",
      "âœ… Recall: 0.6575\n",
      "âœ… F1 Score: 0.7110\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Best Threshold: 0.950\n",
      "âœ… Best Weighted Signal Significance (S/âˆš(S+B)): 7.9495\n",
      "âœ… Total Weighted Surviving Background Events: 66.2589\n",
      "âœ… Weighted False Positives Per Background Type:\n",
      "  HH: 0.0000\n",
      "  ZZ: 0.0000\n",
      "  ZH: 0.3354\n",
      "  WW: 0.0000\n",
      "  tt: 17.6050\n",
      "  qqX: 0.0000\n",
      "  qqqqX: 0.2000\n",
      "  qqHX: 0.5900\n",
      "  qq: 5.7585\n",
      "  pebb: 18.8400\n",
      "  pebbqq: 2.2830\n",
      "  peqqH: 19.7920\n",
      "  pett: 0.8550\n",
      "\n",
      "ðŸŸ¢ Training Stacking Model 3...\n",
      "\n",
      "Fitting LGBM-RF\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Info] Number of positive: 184283, number of negative: 672901\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15710\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214987 -> initscore=-1.295126\n",
      "[LightGBM] [Info] Start training from score -1.295126\n",
      "Fitting LGBM\n",
      "[LightGBM] [Info] Number of positive: 184283, number of negative: 672901\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 15710\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 64 dense feature groups (52.32 MB) transferred to GPU in 0.024675 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.214987 -> initscore=-1.295126\n",
      "[LightGBM] [Info] Start training from score -1.295126\n",
      "Fitting XGBoost\n",
      "Fitting MLP\n",
      "âœ… Models refitted on the new dataset\n",
      "ðŸš€ Training Stacking Model\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [03:10:02] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Accuracy: 0.8838\n",
      "âœ… Recall: 0.6551\n",
      "âœ… F1 Score: 0.7087\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Best Threshold: 0.950\n",
      "âœ… Best Weighted Signal Significance (S/âˆš(S+B)): 8.4433\n",
      "âœ… Total Weighted Surviving Background Events: 46.6617\n",
      "âœ… Weighted False Positives Per Background Type:\n",
      "  HH: 0.0000\n",
      "  ZZ: 1.9737\n",
      "  ZH: 0.2276\n",
      "  WW: 2.5745\n",
      "  tt: 5.0300\n",
      "  qqX: 0.2174\n",
      "  qqqqX: 0.2000\n",
      "  qqHX: 0.5300\n",
      "  qq: 6.6310\n",
      "  pebb: 7.5360\n",
      "  pebbqq: 2.2830\n",
      "  peqqH: 19.1735\n",
      "  pett: 0.2850\n",
      "\n",
      "ðŸŸ¢ Training Stacking Model 4...\n",
      "\n",
      "Fitting LGBM-RF\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "[LightGBM] [Info] Number of positive: 184524, number of negative: 672660\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.152736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 15704\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.215268 -> initscore=-1.293460\n",
      "[LightGBM] [Info] Start training from score -1.293460\n",
      "Fitting LGBM\n",
      "[LightGBM] [Info] Number of positive: 184524, number of negative: 672660\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 15704\n",
      "[LightGBM] [Info] Number of data points in the train set: 857184, number of used features: 64\n",
      "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 64 dense feature groups (52.32 MB) transferred to GPU in 0.024611 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.215268 -> initscore=-1.293460\n",
      "[LightGBM] [Info] Start training from score -1.293460\n",
      "Fitting XGBoost\n",
      "Fitting MLP\n",
      "âœ… Models refitted on the new dataset\n",
      "ðŸš€ Training Stacking Model\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [03:14:31] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training Complete\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Accuracy: 0.8849\n",
      "âœ… Recall: 0.6586\n",
      "âœ… F1 Score: 0.7106\n",
      "[LightGBM] [Warning] feature_fraction is set=0.865132730509119, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.865132730509119\n",
      "[LightGBM] [Warning] bagging_freq is set=8, subsample_freq=0 will be ignored. Current value: bagging_freq=8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.522571778164637, subsample=1.0 will be ignored. Current value: bagging_fraction=0.522571778164637\n",
      "âœ… Best Threshold: 0.932\n",
      "âœ… Best Weighted Signal Significance (S/âˆš(S+B)): 8.0222\n",
      "âœ… Total Weighted Surviving Background Events: 92.2778\n",
      "âœ… Weighted False Positives Per Background Type:\n",
      "  HH: 0.0000\n",
      "  ZZ: 0.9868\n",
      "  ZH: 0.5271\n",
      "  WW: 0.0000\n",
      "  tt: 35.2100\n",
      "  qqX: 0.2174\n",
      "  qqqqX: 0.4000\n",
      "  qqHX: 0.7550\n",
      "  qq: 9.0740\n",
      "  pebb: 3.7680\n",
      "  pebbqq: 3.8050\n",
      "  peqqH: 35.2545\n",
      "  pett: 2.2800\n",
      "\n",
      "Final Results Across 5 Models:\n",
      "Mean Signal Significance: 8.0765\n",
      "Variance of Signal Significance: 0.0450\n",
      "Mean Weighted Background: 72.5251\n",
      "Variance of Weighted Background: 253.9393\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_stacking_model(X, y, event_types, event_weights, run_id):\n",
    "    print(f\"\\nðŸŸ¢ Training Stacking Model {run_id}...\\n\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test, event_train, event_test = train_test_split(\n",
    "        X, y, event_types, test_size=0.2, # random split everytime\n",
    "    )\n",
    "\n",
    "    print(\"Fitting LGBM-RF\")\n",
    "    lgbm_rf_model.fit(X_train, y_train)\n",
    "    print(\"Fitting LGBM\")\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    print(\"Fitting XGBoost\")\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    print(\"Fitting MLP\")\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    print(\"Models refitted on the new dataset\")\n",
    "\n",
    "    meta_xgb = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        colsample_bytree=0.8,\n",
    "        subsample=0.8,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    base_learners = {\n",
    "        \"LGBM_RF\": lgbm_rf_model,\n",
    "        \"LGBM\": lgbm_model,\n",
    "        \"XGBoost\": xgb_model,\n",
    "        \"MLP\": mlp_model\n",
    "    }\n",
    "\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[(name, model) for name, model in base_learners.items()],\n",
    "        final_estimator=meta_xgb,\n",
    "        cv=\"prefit\",\n",
    "        n_jobs=-1, \n",
    "        passthrough=False \n",
    "    )\n",
    "\n",
    "    stacking_model.fit(X_train, y_train)\n",
    "    y_pred = stacking_model.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    \n",
    "    # calculate best signal significance\n",
    "    y_scores = stacking_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    best_threshold = 0\n",
    "    best_significance = 0\n",
    "\n",
    "    for threshold in np.linspace(0.05, 0.95, 50):  \n",
    "        y_pred = (y_scores >= threshold).astype(int)  \n",
    "\n",
    "        signal = sum(\n",
    "            event_weights[event_type] * np.sum((y_pred[event_test == event_type] == 1) & (y_test[event_test == event_type] == 1))\n",
    "            for event_type in event_weights.keys()\n",
    "        )\n",
    "        background = sum(\n",
    "            event_weights[event_type] * np.sum((y_pred[event_test == event_type] == 1) & (y_test[event_test == event_type] == 0))\n",
    "            for event_type in event_weights.keys()\n",
    "        )\n",
    "\n",
    "        if signal + background > 0:\n",
    "            signal_significance = signal / np.sqrt(signal + background)\n",
    "            \n",
    "            if signal_significance > best_significance:\n",
    "                best_significance = signal_significance\n",
    "                best_threshold = threshold\n",
    "\n",
    "    print(f\"Best Threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Best Weighted Signal Significance): {best_significance:.4f}\")\n",
    "\n",
    "    # compute weighted background for chosen threshold\n",
    "    y_pred = (y_scores >= best_threshold).astype(int)  \n",
    "\n",
    "    surviving_background = {event: 0 for event in event_weights.keys()}\n",
    "    total_weighted_FP = 0  \n",
    "\n",
    "    for event_type in event_weights.keys():\n",
    "        weight = event_weights[event_type] # get event weight\n",
    "        event_mask = (event_test.values == event_type) \n",
    "        \n",
    "        FP_count = np.sum((y_pred[event_mask] == 1) & (y_test.values[event_mask] == 0))  # false positives\n",
    "        \n",
    "        weighted_FP = FP_count * weight  \n",
    "        surviving_background[event_type] = weighted_FP\n",
    "        total_weighted_FP += weighted_FP  \n",
    "\n",
    "    print(f\"Total Weighted Surviving Background Events: {total_weighted_FP:.4f}\")\n",
    "\n",
    "    print(\"Weighted False Positives Per Background Type:\")\n",
    "    for event_type, fp_weight in surviving_background.items():\n",
    "        print(f\"  {event_type}: {fp_weight:.4f}\")\n",
    "\n",
    "    return best_significance, total_weighted_FP\n",
    "\n",
    "# TRIALS\n",
    "signal_significance_scores = []\n",
    "weighted_background_scores = []\n",
    "\n",
    "for run_id in range(20):\n",
    "    significance, weighted_background = train_stacking_model(X, y, event_types, event_weights, run_id)\n",
    "    signal_significance_scores.append(significance)\n",
    "    weighted_background_scores.append(weighted_background)\n",
    "\n",
    "mean_signal_significance = np.mean(signal_significance_scores)\n",
    "variance_signal_significance = np.var(signal_significance_scores)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Results Across 5 Models:\")\n",
    "print(f\"Mean Signal Significance: {mean_signal_significance:.4f}\")\n",
    "print(f\"Variance of Signal Significance: {variance_signal_significance:.4f}\")\n",
    "print(f\"Mean Weighted Background: {mean_weighted_background:.4f}\")\n",
    "print(f\"Variance of Weighted Background: {variance_weighted_background:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
