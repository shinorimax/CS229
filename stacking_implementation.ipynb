{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ceac9fb-20c0-44b5-ad46-3c04e272a0d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ combined_dataset_labeled.csv downloaded to combined_dataset_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# download and pre-process dataset to remove 'nParticles'\n",
    "BUCKET_NAME = \"cs229-smoua-stacking\"\n",
    "DATASET_PATH = \"combined_dataset_labeled.csv\"\n",
    "\n",
    "def download_from_gcs(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a file from GCS.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "    print(f\"✅ {source_blob_name} downloaded to {destination_file_name}\")\n",
    "\n",
    "download_from_gcs(BUCKET_NAME, DATASET_PATH, \"combined_dataset_labeled.csv\")\n",
    "\n",
    "df = pd.read_csv(\"combined_dataset_labeled.csv\")\n",
    "df.columns = df.columns.str.strip()  # Remove accidental spaces\n",
    "\n",
    "if \"nParticles\" in df.columns:\n",
    "    df = df.drop(columns=[\"nParticles\"])\n",
    "\n",
    "LABEL_COLUMN = \"label\"\n",
    "EVENT_TYPE_COLUMN = \"event_type\"\n",
    "\n",
    "X = df.drop(columns=[LABEL_COLUMN, EVENT_TYPE_COLUMN])\n",
    "y = df[LABEL_COLUMN]\n",
    "event_types = df[EVENT_TYPE_COLUMN]\n",
    "\n",
    "event_weights = {\n",
    "    \"HH\": 0.0015552 * 1.155 * 5, \"ZZ\": 0.17088 * 1.155 * 5, \"ZH\": 0.00207445 * 1.155 * 5,\n",
    "    \"WW\": 0.5149 * 5, \"tt\": 0.503 * 5, \"qqX\": 0.04347826 * 5, \"qqqqX\": 0.04 * 5, \"qqHX\": 0.001 * 5,\n",
    "    \"qq\": 0.0349 * 5, \"pebb\": 0.7536 * 5, \"pebbqq\": 0.1522 * 5, \"peqqH\": 0.1237 * 5, \"pett\": 0.0570 * 5}\n",
    "\n",
    "X_train, X_test, y_train, y_test, event_train, event_test = train_test_split(\n",
    "    X, y, event_types, test_size=0.2, random_state=50, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7daf399-515e-49a4-ab6d-7b342dc1c27b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find ideal hyperparameters for base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0a0a6-a5fb-4b96-a410-3ede6215149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d289c2-0df1-4bdf-8267-4a861f26a78d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "scale_pos_weight = np.sum(y_train == 0) / np.sum(y_train == 1)  # Background / Signal -- adjusts for class imbalance\n",
    "\n",
    "# search grid for optimal parameters\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"gamma\": [0, 0.1, 0.2],\n",
    "    \"reg_lambda\": [1, 5, 10], \n",
    "    \"scale_pos_weight\": [scale_pos_weight] \n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid_xgb,\n",
    "    n_iter=20,  \n",
    "    scoring=\"recall\", # optimize recall for signal detection\n",
    "    cv=3, \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "print(\"Best XGBoost Parameters:\", random_search_xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e61df2-a029-4c5c-ae36-cb6d4200ab76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "with open(\"random_search_xgb.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(random_search_xgb.best_estimator_, model_file)\n",
    "\n",
    "print(\"Fitted and saved XGBoost model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fa7e17-a811-4572-a010-d260c9b7713a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "\n",
    "def optimize_threshold(model, X_test, y_test, event_test, event_weights, num_thresholds=50):\n",
    "    y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    best_threshold = 0\n",
    "    best_significance = 0\n",
    "    \n",
    "    for threshold in np.linspace(0.05, 0.95, num_thresholds):  \n",
    "        y_pred = (y_scores >= threshold).astype(int)  \n",
    "\n",
    "        signal = 0  # True Positives (TP)\n",
    "        background = 0  # False Positives (FP)\n",
    "\n",
    "        for event_type in event_weights.keys():\n",
    "            weight = event_weights[event_type]  \n",
    "            event_mask = event_test.values == event_type  \n",
    "\n",
    "            # count weighted S (true positives) and B (false positives)\n",
    "            signal += weight * sum((y_pred[event_mask] == 1) & (y_test.values[event_mask] == 1))  # TP\n",
    "            background += weight * sum((y_pred[event_mask] == 1) & (y_test.values[event_mask] == 0))  # FP\n",
    "\n",
    "        if signal + background > 0:\n",
    "            signal_significance = signal / np.sqrt(signal + background)\n",
    "\n",
    "            # track best threshold\n",
    "            if signal_significance > best_significance:\n",
    "                best_significance = signal_significance\n",
    "                best_threshold = threshold\n",
    "    \n",
    "    # Compute final predictions using the best threshold\n",
    "    y_pred_final = (y_scores >= best_threshold).astype(int)\n",
    "    \n",
    "    results = {\n",
    "        \"best_threshold\": best_threshold,\n",
    "        \"best_significance\": best_significance,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_final),\n",
    "        \"recall\": recall_score(y_test, y_pred_final),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_final)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae03c2c4-3e2b-40cb-a8a6-0c4f25c5be0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimize_threshold(random_search_xgb.best_estimator_, X_test, y_test, event_test, event_weights, num_thresholds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156d29ff-4ee1-4143-89e5-24a3f4e8412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file, destination_blob):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob)\n",
    "    blob.upload_from_filename(source_file)\n",
    "    print(f\"✅ Model uploaded to GCS at: gs://{bucket_name}/{destination_blob}\")\n",
    "\n",
    "upload_to_gcs(BUCKET_NAME, \"random_search_xgb.pkl\", \"models/random_search_xgb.pkl\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4323c-fd57-4934-b04d-0e341d225975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12521e86-3ab9-4635-a17f-13321e6de595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 15),  \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 100), \n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.7, 1.0),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.7, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1, 10, log=True),  \n",
    "        \"scale_pos_weight\": scale_pos_weight, \n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\"]),  \n",
    "        \"device\": \"gpu\"  \n",
    "    }\n",
    "\n",
    "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=50, stratify=y_train)\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train_sub, y_train_sub,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"logloss\",\n",
    "        callbacks=[lgb.early_stopping(20, verbose=False)]  \n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    recall = np.sum((y_pred == 1) & (y_val == 1)) / np.sum(y_val == 1)  # optimize for recall\n",
    "    \n",
    "    return recall\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)  \n",
    "\n",
    "print(\"Best LightGBM Parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb40c7-6588-40e2-bc7b-b35c0ddeb95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lgbm_params = { # copy and pasted from prev optimization study\n",
    "    \"n_estimators\": 174,\n",
    "    \"max_depth\": 0, \n",
    "    \"learning_rate\": 0.0822916699309106,\n",
    "    \"num_leaves\": 69,\n",
    "    \"colsample_bytree\": 0.8942260280757528,\n",
    "    \"subsample\": 0.7992004746355641,\n",
    "    \"reg_lambda\": 7.765769244511278,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"random_state\": 42,\n",
    "    \"device\": \"gpu\"  \n",
    "}\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(**best_lgbm_params)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "with open(\"lgbm_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(lgbm_model, model_file)\n",
    "\n",
    "upload_to_gcs(BUCKET_NAME, \"lgbm_model.pkl\", \"models/lgbm_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f9224-bfbe-4f26-9cfb-0deecfec1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_threshold(lgbm_model, X_test, y_test, event_test, event_weights, num_thresholds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640076d0-24fc-4938-b8c4-ce5e1045571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm (random forest) -- randomforestclassifier took too long to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12e7d9-5956-4981-a242-ee5d3a3d97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"boosting_type\": \"rf\",  # random forest\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
    "        \"max_depth\": trial.suggest_categorical(\"max_depth\", [-1, 5, 10, 15]),  \n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),  \n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),  \n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10), \n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 0.1, 10), \n",
    "        \"scale_pos_weight\": scale_pos_weight,  \n",
    "        \"device\": \"gpu\",  \n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    recall = np.sum((y_pred == 1) & (y_val == 1)) / np.sum(y_val == 1)  # optimize for recall\n",
    "    return recall\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  \n",
    "\n",
    "print(\"Best LightGBM (RF) Parameters:\", study.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681276de-4376-4b85-9e96-a65ba82c1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_rf_params = {\n",
    "    \"n_estimators\": 163,\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 100,\n",
    "    \"feature_fraction\": 0.865132730509119, \n",
    "    \"bagging_fraction\": 0.522571778164637, \n",
    "    \"bagging_freq\": 8, \n",
    "    \"learning_rate\": 0.042053606399311175,\n",
    "    \"reg_lambda\": 0.43456898133748256,  \n",
    "    \"boosting_type\": \"rf\", \n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "lgbm_rf_model = lgb.LGBMClassifier(**lgbm_rf_params)\n",
    "lgbm_rf_model.fit(X_train, y_train)\n",
    "\n",
    "with open(\"lgbm_rf_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(lgbm_rf_model, model_file)\n",
    "\n",
    "upload_to_gcs(BUCKET_NAME, \"lgbm_rf_model.pkl\", \"models/lgbm_rf_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0a3912-8d28-4aa9-9b31-8b424bf124cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_threshold(lgbm_rf_model, X_test, y_test, event_test, event_weights, num_thresholds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97837bc0-72a6-4a0d-9861-99cacb8e5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c2d4f-dc4c-42ab-a53b-3429e1892f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# training in pytorch because it's faster\n",
    "X_train_torch = torch.tensor(X_train.values, dtype=torch.float32).cuda()\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.long).cuda()\n",
    "\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).cuda()\n",
    "\n",
    "X_train_np = X_train_torch.cpu().numpy()  \n",
    "y_train_np = y_train_torch.cpu().numpy()\n",
    "\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "    X_train_np, y_train_np, test_size=0.1, random_state=42, stratify=y_train_np\n",
    ")\n",
    "\n",
    "X_train_sub = torch.tensor(X_train_sub, dtype=torch.float32).cuda()\n",
    "y_train_sub = torch.tensor(y_train_sub, dtype=torch.long).cuda()\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).cuda()\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).cuda()\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden1, hidden2, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden1)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(hidden2, 2)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)  \n",
    "\n",
    "def objective(trial):\n",
    "    hidden1 = trial.suggest_int(\"hidden1\", 128, 512)\n",
    "    hidden2 = trial.suggest_int(\"hidden2\", 64, 256)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.05, 0.3)  \n",
    "    lr = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-3)\n",
    "    l2_reg = trial.suggest_loguniform(\"l2_reg\", 1e-5, 1e-3)\n",
    "\n",
    "    model = MLP(X_train_sub.shape[1], hidden1, hidden2, dropout).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).cuda()  \n",
    "\n",
    "    for epoch in range(15):  # 15 epochs to reduce training time\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train_sub)\n",
    "        loss = criterion(outputs, y_train_sub)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs_val = model(X_val)\n",
    "        probs = torch.softmax(outputs_val, dim=1)[:, 1]\n",
    "\n",
    "        best_significance = 0\n",
    "        best_threshold = 0\n",
    "        for threshold in np.linspace(0.01, 0.99, 50):\n",
    "            preds = (probs >= threshold).int()\n",
    "\n",
    "            TP = (preds[y_val == 1] == 1).sum().item()  \n",
    "            FP = (preds[y_val == 0] == 1).sum().item()  \n",
    "\n",
    "            if TP + FP > 0:\n",
    "                significance = TP / np.sqrt(TP + FP) \n",
    "                if significance > best_significance:\n",
    "                    best_significance = significance\n",
    "                    best_threshold = threshold\n",
    "    \n",
    "    return best_significance  # maximize signal significance\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  # Run 20 trials\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best MLP Parameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c76e9-fc10-4d09-b685-d1a050138c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "best_params_sklearn = {\n",
    "    \"hidden1\": 317,  \n",
    "    \"hidden2\": 122, \n",
    "    \"alpha\": 3.0687963534524235e-05, \n",
    "    \"learning_rate_init\": 0.0008421984089042774, \n",
    "}\n",
    "\n",
    "mlp_sklearn = MLPClassifier(\n",
    "    hidden_layer_sizes=(best_params_sklearn[\"hidden1\"], best_params_sklearn[\"hidden2\"]),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    alpha=best_params_sklearn[\"alpha\"],  \n",
    "    learning_rate_init=best_params_sklearn[\"learning_rate_init\"],\n",
    "    max_iter=100,  \n",
    "    random_state=42,\n",
    "    warm_start=True,\n",
    "    early_stopping=True,  \n",
    "    validation_fraction=0.1  \n",
    ")\n",
    "\n",
    "mlp_sklearn.fit(X_train, y_train)\n",
    "\n",
    "with open(\"mlp_sklearn.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(mlp_sklearn, model_file)\n",
    "\n",
    "upload_to_gcs(BUCKET_NAME, \"mlp_sklearn.pkl\", \"models/mlp_sklearn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ec863-3641-4cbf-9f7b-b79a15c65423",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_threshold(mlp_sklearn, X_test, y_test, event_test, event_weights, num_thresholds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc8f708-7b80-4797-aefa-6ba4ab962078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b8f6c-7430-46a1-a94b-111f74831141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_from_gcs(bucket_name, model_path):\n",
    "    \"\"\"Loads a model from Google Cloud Storage.\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(model_path)\n",
    "\n",
    "    model_bytes = blob.download_as_bytes()\n",
    "    model = pickle.loads(model_bytes)\n",
    "    print(f\"Loaded model from gs://{bucket_name}/{model_path}\")\n",
    "    return model\n",
    "\n",
    "lgbm_rf_model = load_model_from_gcs(BUCKET_NAME, \"models/lgbm_rf_model.pkl\")\n",
    "lgbm_model = load_model_from_gcs(BUCKET_NAME, \"models/lgbm_model.pkl\")\n",
    "xgb_model = load_model_from_gcs(BUCKET_NAME, \"models/random_search_xgb.pkl\")\n",
    "mlp_model = load_model_from_gcs(BUCKET_NAME, \"models/mlp_sklearn.pkl\")\n",
    "\n",
    "base_learners = {\n",
    "    \"LGBM_RF\": lgbm_rf_model,\n",
    "    \"LGBM\": lgbm_model,\n",
    "    \"XGBoost\": xgb_model,\n",
    "    \"MLP\": mlp_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be3db9-ac6f-4c04-a801-bdccea35acd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "meta_xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[(name, model) for name, model in base_learners.items()],\n",
    "    final_estimator=meta_xgb,\n",
    "    cv=\"prefit\", # already trained models\n",
    "    n_jobs=-1,  \n",
    "    passthrough=False  # use only base model predictions as inputs to meta model\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bff3b-9ad2-4b6e-9a35-54808352f3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_threshold(stacking_model, X_test, y_test, event_test, event_weights, num_thresholds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57655fb-fc14-4a8a-bfb5-b6595f2efbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stacking_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(stacking_model, model_file)\n",
    "    \n",
    "upload_to_gcs(BUCKET_NAME, \"stacking_model.pkl\", \"models/stacking_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03d7474-6853-4989-92ac-5d5922ce7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655fc9c-fc5b-4e7a-8abe-554b2870d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_scores = stacking_model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "signal_scores = y_scores[y_test == 1]\n",
    "background_scores = y_scores[y_test == 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(signal_scores, bins=50, color=\"red\", alpha=0.5, label=\"Signal\", kde=True, stat=\"count\")\n",
    "sns.histplot(background_scores, bins=50, color=\"blue\", alpha=0.5, label=\"Background\", kde=True, stat=\"count\")\n",
    "\n",
    "optimal_threshold = 0.932 # ADJUST FOR DIFFERENT TRIALS\n",
    "plt.axvline(optimal_threshold, color=\"black\", linestyle=\"dotted\", linewidth=2, label=f\"Optimal Threshold = {optimal_threshold}\")\n",
    "\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Stacking Model Output Probability Distribution\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61eff6d-4549-4c1c-89b1-e92e2c3dd353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr) \n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f\"ROC Curve (AUC = {roc_auc:.4f})\")  \n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)  \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ac10e-4391-4f56-99c0-444d3871b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = stacking_model.predict_proba(X_test)[:, 1]  \n",
    "y_pred = (y_scores >= optimal_threshold).astype(int)  \n",
    "\n",
    "true_positives = np.sum((y_pred == 1) & (y_test.values == 1))  \n",
    "false_positives = np.sum((y_pred == 1) & (y_test.values == 0))  \n",
    "false_negatives = np.sum((y_pred == 0) & (y_test.values == 1))  \n",
    "true_negatives = np.sum((y_pred == 0) & (y_test.values == 0))  \n",
    "\n",
    "tn_by_type = {event: 0 for event in np.unique(event_test.values)}  \n",
    "\n",
    "for event_type in np.unique(event_test.values):\n",
    "    event_mask = (event_test.values == event_type)\n",
    "    tn_by_type[event_type] = np.sum((y_pred[event_mask] == 0) & (y_test.values[event_mask] == 0))  \n",
    "\n",
    "print(f\"True Positives (TP): {true_positives}\")\n",
    "print(f\"False Positives (FP): {false_positives}\")\n",
    "print(f\"False Negatives (FN): {false_negatives}\")\n",
    "print(f\"True Negatives (TN): {true_negatives}\")\n",
    "\n",
    "print(\"\\n True Negatives Per Background Type:\")\n",
    "for event_type, tn_count in tn_by_type.items():\n",
    "    print(f\"  {event_type}: {tn_count}\")\n",
    "\n",
    "conf_matrix = np.array([[true_negatives, false_positives],\n",
    "                        [false_negatives, true_positives]])\n",
    "\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=[\"Actual Negative\", \"Actual Positive\"], \n",
    "                              columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix_df, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(f\"Confusion Matrix (Threshold {optimal_threshold})\")\n",
    "plt.ylabel(\"Actual Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080a97e-bd67-4d16-83f7-52439460a4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
