{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm.notebook import tqdm  # Use notebook version for Jupyter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from deap import base, creator, tools, algorithms\n",
    "from sklearn.metrics import log_loss\n",
    "import joblib\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 9999\n",
    "random.seed(SEED)  # Ensures consistent behavior in DEAP\n",
    "np.random.seed(SEED)  # Ensures NumPy-based operations are reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder containing the raw dataset files\n",
    "raw_data_folder = \"raw dataset\"  # Update with your actual folder path\n",
    "\n",
    "# Get all file names in the folder\n",
    "all_files = sorted([f for f in os.listdir(raw_data_folder) if f.endswith(\".csv\")])\n",
    "\n",
    "# Identify signal and background files\n",
    "signal_file = [f for f in all_files if \"signal\" in f.lower()][0]  # Assumes \"signal\" is in filename\n",
    "background_files = sorted([f for f in all_files if \"B\" in f.upper()])  # Assumes \"B\" in filename means background\n",
    "\n",
    "# Load the signal dataset and add a label column\n",
    "signal_df = pd.read_csv(os.path.join(raw_data_folder, signal_file))\n",
    "# Drop 'nParticles' if it exists\n",
    "if 'nParticles' in signal_df.columns:\n",
    "    signal_df.drop(columns=['nParticles'], inplace=True)\n",
    "signal_df[\"label\"] = 1  # Assign label 1 for signal events\n",
    "\n",
    "# Load background datasets and add a label column\n",
    "background_dfs = []\n",
    "background_labels = []  # Store filenames for indexing reference\n",
    "background_types = []  # Store background type\n",
    "\n",
    "# Define weights for each background type\n",
    "background_weights = {\n",
    "    \"HH\": 0.0015552 * 1.155,\n",
    "    \"qq\": 0.0349,\n",
    "    \"tt\": 0.503,\n",
    "    \"ZZ\": 0.17088 * 1.155,\n",
    "    \"WW\": 0.5149,\n",
    "    \"qqX\": 0.04347826,\n",
    "    \"qqqqX\": 0.04,\n",
    "    \"qqHX\": 0.001,\n",
    "    \"ZH\": 0.00207445 * 1.155,\n",
    "    \"pebb\": 0.7536,\n",
    "    \"pebbqq\": 0.1522,\n",
    "    \"peqqH\": 0.1237,\n",
    "    \"pett\": 0.0570,\n",
    "}\n",
    "\n",
    "# Apply reweighting factor for the test set\n",
    "test_size = 0.25\n",
    "reweight_factor = 1 / test_size  # = 4.0\n",
    "background_weights = {k: v * reweight_factor for k, v in background_weights.items()}\n",
    "\n",
    "for idx, bg_file in enumerate(background_files):\n",
    "    bg_df = pd.read_csv(os.path.join(raw_data_folder, bg_file))\n",
    "\n",
    "    # Drop 'nParticles' if it exists\n",
    "    if 'nParticles' in bg_df.columns:\n",
    "        bg_df.drop(columns=['nParticles'], inplace=True)\n",
    "\n",
    "    bg_df[\"label\"] = 0  # Assign label 0 for background events\n",
    "\n",
    "    # Extract background type from filename (remove \"B\" and \".csv\")\n",
    "    bg_type = bg_file[1:].replace(\".csv\", \"\")\n",
    "    bg_df[\"background_type\"] = bg_type  # Store background type\n",
    "\n",
    "    background_dfs.append(bg_df)\n",
    "    background_labels.append(bg_file)  # Store file name for reference\n",
    "    background_types.append(bg_type)\n",
    "\n",
    "# Drop 'background_type' column before extracting features\n",
    "X_signal = signal_df.drop(columns=[\"label\"], errors=\"ignore\")  # Ensure label is dropped\n",
    "y_signal = signal_df[\"label\"]  # Extract labels\n",
    "\n",
    "X_backgrounds = [bg.drop(columns=[\"label\", \"background_type\"], errors=\"ignore\") for bg in background_dfs]  # Drop extra columns\n",
    "y_backgrounds = [bg[\"label\"] for bg in background_dfs]  # Extract labels correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Datasets and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the split datasets\n",
    "os.makedirs('split_datasets', exist_ok=True)\n",
    "\n",
    "# First, create a consistent three-way split for the signal dataset\n",
    "X_train_val_signal, X_test_signal, y_train_val_signal, y_test_signal = train_test_split(\n",
    "    X_signal, y_signal, test_size=test_size, random_state=SEED, stratify=y_signal\n",
    ")\n",
    "\n",
    "X_train_signal, X_val_signal, y_train_signal, y_val_signal = train_test_split(\n",
    "    X_train_val_signal, y_train_val_signal, test_size=0.2, random_state=SEED, stratify=y_train_val_signal\n",
    ")\n",
    "\n",
    "# Save signal splits\n",
    "joblib.dump(X_train_signal, 'split_datasets/X_train_signal.pkl')\n",
    "joblib.dump(X_val_signal, 'split_datasets/X_val_signal.pkl')\n",
    "joblib.dump(X_test_signal, 'split_datasets/X_test_signal.pkl')\n",
    "joblib.dump(y_train_signal, 'split_datasets/y_train_signal.pkl')\n",
    "joblib.dump(y_val_signal, 'split_datasets/y_val_signal.pkl')\n",
    "joblib.dump(y_test_signal, 'split_datasets/y_test_signal.pkl')\n",
    "\n",
    "# Prepare to store test background types\n",
    "background_types_train = []\n",
    "background_types_val = []\n",
    "background_types_test = []\n",
    "\n",
    "# Split and save each background dataset\n",
    "for i, bg_label in enumerate(background_labels):\n",
    "    clean_name = bg_label.replace('.csv', '').replace(' ', '_')\n",
    "\n",
    "    X_bg = X_backgrounds[i]\n",
    "    y_bg = y_backgrounds[i]\n",
    "\n",
    "    X_train_val_bg, X_test_bg, y_train_val_bg, y_test_bg = train_test_split(\n",
    "        X_bg, y_bg, test_size=test_size, random_state=SEED, stratify=y_bg\n",
    "    )\n",
    "\n",
    "    X_train_bg, X_val_bg, y_train_bg, y_val_bg = train_test_split(\n",
    "        X_train_val_bg, y_train_val_bg, test_size=0.2, random_state=SEED, stratify=y_train_val_bg\n",
    "    )\n",
    "\n",
    "    # Store background types for test set\n",
    "    background_types_train.extend([background_labels[i][1:].replace(\".csv\", \"\")] * len(X_train_bg))\n",
    "    background_types_val.extend([background_labels[i][1:].replace(\".csv\", \"\")] * len(X_val_bg))\n",
    "    background_types_test.extend([background_labels[i][1:].replace(\".csv\", \"\")] * len(X_test_bg))\n",
    "\n",
    "    # Save background splits\n",
    "    joblib.dump(X_train_bg, f'split_datasets/X_train_{clean_name}.pkl')\n",
    "    joblib.dump(X_val_bg, f'split_datasets/X_val_{clean_name}.pkl')\n",
    "    joblib.dump(X_test_bg, f'split_datasets/X_test_{clean_name}.pkl')\n",
    "    joblib.dump(y_train_bg, f'split_datasets/y_train_{clean_name}.pkl')\n",
    "    joblib.dump(y_val_bg, f'split_datasets/y_val_{clean_name}.pkl')\n",
    "    joblib.dump(y_test_bg, f'split_datasets/y_test_{clean_name}.pkl')\n",
    "\n",
    "    print(f\"Saved splits for background {i}: {bg_label}\")\n",
    "\n",
    "# Save background types for the test set\n",
    "joblib.dump(background_types_train, 'split_datasets/background_types_train.pkl')\n",
    "joblib.dump(background_types_val, 'split_datasets/background_types_val.pkl')\n",
    "joblib.dump(background_types_test, 'split_datasets/background_types_test.pkl')\n",
    "\n",
    "print(\"All dataset splits have been saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert list to numpy array\n",
    "# background_array = np.array(background_types_test)\n",
    "\n",
    "# # Get unique background types and their counts\n",
    "# unique_types, counts = np.unique(background_array, return_counts=True)\n",
    "\n",
    "# # Print each background type and its count\n",
    "# for bg_type, count in zip(unique_types, counts):\n",
    "#     print(f\"{bg_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Simple BTD Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BTD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load signal training data\n",
    "X_train_signal = joblib.load('split_datasets/X_train_signal.pkl')\n",
    "y_train_signal = joblib.load('split_datasets/y_train_signal.pkl')\n",
    "\n",
    "# Training hyperparameters\n",
    "depth = 2\n",
    "n = 100\n",
    "lr = 0.1\n",
    "\n",
    "print(\"\\nStarting Parallel BDT Training...\\n\")\n",
    "\n",
    "def train_model(i, background_label):\n",
    "    \"\"\"Function to train a single BDT model on one background dataset.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load background training data\n",
    "    clean_name = background_label.replace('.csv', '').replace(' ', '_')\n",
    "    X_train_bg = joblib.load(f'split_datasets/X_train_{clean_name}.pkl')\n",
    "    y_train_bg = joblib.load(f'split_datasets/y_train_{clean_name}.pkl')\n",
    "\n",
    "    # Combine signal + one background dataset for training\n",
    "    X_train_combined = pd.concat([X_train_signal, X_train_bg])\n",
    "    y_train_combined = np.concatenate([y_train_signal, y_train_bg])\n",
    "\n",
    "    # Train a Boosted Decision Tree (BDT)\n",
    "    bdt = AdaBoostClassifier(\n",
    "        estimator=DecisionTreeClassifier(max_depth=depth),\n",
    "        n_estimators=n,\n",
    "        learning_rate=lr,\n",
    "        algorithm=\"SAMME\"\n",
    "    )\n",
    "    bdt.fit(X_train_combined, y_train_combined)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    return bdt, f\"âœ” Model {i+1}/12 trained on {background_label} (Time: {elapsed_time:.2f} sec)\"\n",
    "\n",
    "# ** Train models in parallel **\n",
    "num_jobs = min(12, joblib.cpu_count())  # Limit to available CPU cores\n",
    "results = Parallel(n_jobs=num_jobs)(\n",
    "    delayed(train_model)(i, background_labels[i]) for i in range(12)\n",
    ")\n",
    "\n",
    "# Unpack trained models and messages\n",
    "trained_models, messages = zip(*results)\n",
    "\n",
    "# Print training messages\n",
    "for msg in messages:\n",
    "    tqdm.write(msg)\n",
    "\n",
    "tqdm.write(\"\\n Parallel Training Complete! All models are ready.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save BTD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save models\n",
    "model_dir = \"models\"\n",
    "os.makedirs(model_dir, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Save each trained model with detailed filename\n",
    "for i, model in enumerate(trained_models):\n",
    "    filename = f\"bdt_model_bg{i+1}_depth{depth}_n{n}_lr{lr}.joblib\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    joblib.dump(model, filepath)\n",
    "    print(f\"Model {i+1} saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check BTD Performance on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load signal test data\n",
    "X_test_signal = joblib.load('split_datasets/X_test_signal.pkl')\n",
    "y_test_signal = joblib.load('split_datasets/y_test_signal.pkl')\n",
    "\n",
    "# Load background test datasets\n",
    "X_test_backgrounds = []\n",
    "y_test_backgrounds = []\n",
    "\n",
    "for bg_file in background_labels:\n",
    "    clean_name = bg_file.replace('.csv', '').replace(' ', '_')\n",
    "    X_test_bg = joblib.load(f'split_datasets/X_test_{clean_name}.pkl')\n",
    "    y_test_bg = joblib.load(f'split_datasets/y_test_{clean_name}.pkl')\n",
    "    \n",
    "    X_test_backgrounds.append(X_test_bg)\n",
    "    y_test_backgrounds.append(y_test_bg)\n",
    "\n",
    "# Initialize a 12x13 matrix to store results\n",
    "output_matrix = np.zeros((12, 13))\n",
    "\n",
    "# Evaluate each trained BDT model on the test datasets\n",
    "for model_idx, model in enumerate(trained_models):\n",
    "    for dataset_idx, dataset in enumerate([X_test_signal] + X_test_backgrounds):  \n",
    "        # Get predicted probability of being signal\n",
    "        predictions = model.predict_proba(dataset)[:, 1]  # Extract P(class=1) (signal probability)\n",
    "        \n",
    "        # Store the average probability of being signal on **test dataset only**\n",
    "        output_matrix[model_idx, dataset_idx] = np.mean(predictions)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "datasets = [\"Signal Test\"] + [f\"Background {i+1} Test\" for i in range(12)]\n",
    "model_labels = [f\"Model {i+1}\" for i in range(12)]\n",
    "\n",
    "df_results = pd.DataFrame(output_matrix, index=model_labels, columns=datasets)\n",
    "\n",
    "# Define a function to apply conditional formatting\n",
    "def highlight_matrix(df):\n",
    "    styles = pd.DataFrame(\"\", index=df.index, columns=df.columns)  # Initialize with empty styles\n",
    "\n",
    "    # Highlight the first column\n",
    "    styles.iloc[:, 0] = \"background-color: yellow\"\n",
    "\n",
    "    # Highlight diagonal [1,2], [2,3], ..., [12,13]\n",
    "    for i in range(min(len(df.index), len(df.columns) - 1)):\n",
    "        styles.iloc[i, i + 1] = \"background-color: lightblue\"\n",
    "\n",
    "    return styles\n",
    "\n",
    "# Apply the styling function to df_results\n",
    "styled_df = df_results.style.apply(highlight_matrix, axis=None)\n",
    "\n",
    "styled_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train XGB Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 2\n",
    "n = 100\n",
    "lr = 0.1\n",
    "\n",
    "# Store trained models and test sets\n",
    "trained_xgb_models = []\n",
    "train_test_splits = []\n",
    "\n",
    "# Load signal training data\n",
    "X_train_signal = joblib.load('split_datasets/X_train_signal.pkl')\n",
    "y_train_signal = joblib.load('split_datasets/y_train_signal.pkl')\n",
    "\n",
    "print(\"\\nStarting XGBoost Training...\\n\")\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "with tqdm(total=12, desc=\"Training Progress\", unit=\"model\", leave=True) as pbar:\n",
    "    for i in range(12):\n",
    "        start_time = time.time()  # Track time for each model\n",
    "\n",
    "        # Clean filename for loading\n",
    "        clean_name = background_labels[i].replace('.csv', '').replace(' ', '_')\n",
    "        \n",
    "        # Load background training data\n",
    "        X_train_bg = joblib.load(f'split_datasets/X_train_{clean_name}.pkl')\n",
    "        y_train_bg = joblib.load(f'split_datasets/y_train_{clean_name}.pkl')\n",
    "        # X_test_bg = joblib.load(f'split_datasets/X_test_{clean_name}.pkl')\n",
    "        # y_test_bg = joblib.load(f'split_datasets/y_test_{clean_name}.pkl')\n",
    "\n",
    "        # Combine signal + one background dataset\n",
    "        X_train_combined = pd.concat([X_train_signal, X_train_bg])\n",
    "        y_train_combined = np.concatenate([y_train_signal, y_train_bg])\n",
    "\n",
    "        # Split into train (75%) and test (25%)\n",
    "        # X_train, X_test, y_train, y_test = train_test_split(X_combined, y_combined, test_size=0.25, random_state=42)\n",
    "\n",
    "        # Convert to XGBoost DMatrix (optimized for speed)\n",
    "        dtrain = xgb.DMatrix(X_train_combined, label=y_train_combined)\n",
    "        # dtest = xgb.DMatrix(X_test_, label=y_test)\n",
    "\n",
    "        # Define XGBoost parameters\n",
    "        xgb_params = {\n",
    "            \"objective\": \"binary:logistic\",  # Binary classification\n",
    "            \"eval_metric\": \"logloss\",  # Log-loss for binary classification\n",
    "            \"max_depth\": depth,  # Similar to BDT depth\n",
    "            \"learning_rate\": lr,  # Step size\n",
    "            \"n_estimators\": n,  # Number of boosting rounds\n",
    "            \"tree_method\": \"hist\",  # Optimized for speed\n",
    "        }\n",
    "\n",
    "        # Train XGBoost model\n",
    "        xgb_model = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=100)\n",
    "\n",
    "        # Store trained model and test data\n",
    "        trained_xgb_models.append(xgb_model)\n",
    "        # train_test_splits.append((X_test, y_test))\n",
    "\n",
    "        # Print progress without interfering with tqdm\n",
    "        elapsed_time = time.time() - start_time\n",
    "        tqdm.write(f\"Model {i+1}/12 trained on {background_labels[i]} (Time: {elapsed_time:.2f} sec)\")\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "print(\"\\nTraining Complete! All models are ready.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save BTD Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder to save models\n",
    "model_dir = \"xgb_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "# Save each trained XGBoost model\n",
    "for i, model in enumerate(trained_xgb_models):\n",
    "    filename = f\"xgb_model_bg{i+1}_depth{depth}_n{n}_lr{lr}.model\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "    model.save_model(filepath)\n",
    "    print(f\"Model {i+1} saved to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check XGB Performance on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load signal test data\n",
    "X_test_signal = joblib.load('split_datasets/X_test_signal.pkl')\n",
    "y_test_signal = joblib.load('split_datasets/y_test_signal.pkl')\n",
    "\n",
    "# Load background test datasets\n",
    "X_test_backgrounds = []\n",
    "y_test_backgrounds = []\n",
    "\n",
    "for bg_file in background_labels:\n",
    "    clean_name = bg_file.replace('.csv', '').replace(' ', '_')\n",
    "    X_test_bg = joblib.load(f'split_datasets/X_test_{clean_name}.pkl')\n",
    "    y_test_bg = joblib.load(f'split_datasets/y_test_{clean_name}.pkl')\n",
    "    \n",
    "    X_test_backgrounds.append(X_test_bg)\n",
    "    y_test_backgrounds.append(y_test_bg)\n",
    "\n",
    "# Initialize a 12x13 matrix to store results\n",
    "output_matrix = np.zeros((12, 13))\n",
    "\n",
    "# Evaluate each trained XGBoost model on the test datasets\n",
    "for model_idx, model in enumerate(trained_xgb_models):\n",
    "    for dataset_idx, dataset in enumerate([X_test_signal] + X_test_backgrounds):  \n",
    "        # Convert dataset to XGBoost DMatrix (necessary for prediction)\n",
    "        dmatrix = xgb.DMatrix(dataset)\n",
    "        \n",
    "        # Get predicted probability (XGBoost automatically returns probabilities for binary classification)\n",
    "        predictions = model.predict(dmatrix)\n",
    "        \n",
    "        # Store the average probability of being signal on **test dataset only**\n",
    "        output_matrix[model_idx, dataset_idx] = np.mean(predictions)  # Mean probability\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "datasets = [\"Signal Test\"] + [f\"Background {i+1} Test\" for i in range(12)]\n",
    "model_labels = [f\"Model {i+1}\" for i in range(12)]\n",
    "\n",
    "df_results = pd.DataFrame(output_matrix, index=model_labels, columns=datasets)\n",
    "\n",
    "# Apply the styling function to df_results\n",
    "styled_df = df_results.style.apply(highlight_matrix, axis=None)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained BDT and XGB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BDT models\n",
    "bdt_models = []\n",
    "for i in range(12):\n",
    "    filename = f\"bdt_model_bg{i+1}_depth2_n100_lr0.1.joblib\"\n",
    "    filepath = os.path.join(\"models\", filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        model = joblib.load(filepath)\n",
    "        bdt_models.append(model)\n",
    "        print(f\"Loaded BDT Model {i+1} from {filepath}\")\n",
    "    else:\n",
    "        print(f\"Model {i+1} not found, you may need to train it first.\")\n",
    "\n",
    "# Load XGBoost models\n",
    "xgb_models = []\n",
    "for i in range(12):\n",
    "    filename = f\"xgb_model_bg{i+1}_depth2_n100_lr0.1.model\"\n",
    "    filepath = os.path.join(\"xgb_models\", filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        model = xgb.Booster()\n",
    "        model.load_model(filepath)\n",
    "        xgb_models.append(model)\n",
    "        print(f\"Loaded XGBoost Model {i+1} from {filepath}\")\n",
    "    else:\n",
    "        print(f\"Model {i+1} not found, you may need to train it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load signal validation data\n",
    "X_val_signal = joblib.load('split_datasets/X_val_signal.pkl')\n",
    "y_val_signal = joblib.load('split_datasets/y_val_signal.pkl')\n",
    "\n",
    "# Load background validation datasets\n",
    "X_val_backgrounds = []\n",
    "y_val_backgrounds = []\n",
    "\n",
    "for bg_file in background_labels:\n",
    "    clean_name = bg_file.replace('.csv', '').replace(' ', '_')\n",
    "    X_val_bg = joblib.load(f'split_datasets/X_val_{clean_name}.pkl')\n",
    "    y_val_bg = joblib.load(f'split_datasets/y_val_{clean_name}.pkl')\n",
    "    \n",
    "    X_val_backgrounds.append(X_val_bg)\n",
    "    y_val_backgrounds.append(y_val_bg)\n",
    "\n",
    "# Load signal test data\n",
    "X_test_signal = joblib.load('split_datasets/X_test_signal.pkl')\n",
    "y_test_signal = joblib.load('split_datasets/y_test_signal.pkl')\n",
    "\n",
    "# Load background test datasets\n",
    "X_test_backgrounds = []\n",
    "y_test_backgrounds = []\n",
    "\n",
    "for bg_file in background_labels:\n",
    "    clean_name = bg_file.replace('.csv', '').replace(' ', '_')\n",
    "    X_test_bg = joblib.load(f'split_datasets/X_test_{clean_name}.pkl')\n",
    "    y_test_bg = joblib.load(f'split_datasets/y_test_{clean_name}.pkl')\n",
    "    \n",
    "    X_test_backgrounds.append(X_test_bg)\n",
    "    y_test_backgrounds.append(y_test_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply BTD / XGB on Validation and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize storage for NN datasets\n",
    "X_nn_bdt_train, X_nn_xgb_train = [], []\n",
    "y_nn_train = []\n",
    "\n",
    "# Function to extract model outputs\n",
    "def get_model_outputs(models, dataset_X):\n",
    "    outputs = []\n",
    "    for model in models:\n",
    "        if isinstance(model, xgb.Booster):  # XGBoost models\n",
    "            dmatrix = xgb.DMatrix(dataset_X)\n",
    "            outputs.append(model.predict(dmatrix))  # Direct prediction for XGBoost\n",
    "        else:  # BDT models (Scikit-Learn)\n",
    "            outputs.append(model.predict_proba(dataset_X)[:, 1])  # Probability of being signal\n",
    "    return outputs\n",
    "\n",
    "# Apply models to validation datasets (for NN training)\n",
    "for dataset_idx, (X_val, y_val) in enumerate(\n",
    "    zip([X_val_signal] + X_val_backgrounds, [y_val_signal] + y_val_backgrounds)\n",
    "):\n",
    "    # Extract BDT and XGB outputs separately\n",
    "    bdt_train_features = np.column_stack(get_model_outputs(bdt_models, X_val))\n",
    "    xgb_train_features = np.column_stack(get_model_outputs(xgb_models, X_val))\n",
    "\n",
    "    # Store separately\n",
    "    X_nn_bdt_train.append(bdt_train_features)\n",
    "    X_nn_xgb_train.append(xgb_train_features)\n",
    "    y_nn_train.append(y_val.to_numpy())\n",
    "    \n",
    "# Convert lists to single NumPy arrays\n",
    "X_nn_bdt_train = np.vstack(X_nn_bdt_train)\n",
    "X_nn_xgb_train = np.vstack(X_nn_xgb_train)\n",
    "y_nn_train = np.concatenate(y_nn_train)\n",
    "\n",
    "\n",
    "# Initialize storage for NN test datasets\n",
    "X_nn_bdt_test, X_nn_xgb_test = [], []\n",
    "y_nn_test = []\n",
    "\n",
    "# Apply models to test datasets (for finding optimal threshold)\n",
    "for dataset_idx, (X_test, y_test) in enumerate(\n",
    "    zip([X_test_signal] + X_test_backgrounds, [y_test_signal] + y_test_backgrounds)\n",
    "):\n",
    "    # Extract BDT and XGB outputs separately\n",
    "    bdt_test_features = np.column_stack(get_model_outputs(bdt_models, X_test))\n",
    "    xgb_test_features = np.column_stack(get_model_outputs(xgb_models, X_test))\n",
    "\n",
    "    # Store separately\n",
    "    X_nn_bdt_test.append(bdt_test_features)\n",
    "    X_nn_xgb_test.append(xgb_test_features)\n",
    "    y_nn_test.append(y_test.to_numpy())\n",
    "\n",
    "# Convert lists to single NumPy arrays\n",
    "X_nn_bdt_test = np.vstack(X_nn_bdt_test)\n",
    "X_nn_xgb_test = np.vstack(X_nn_xgb_test)\n",
    "y_nn_test = np.concatenate(y_nn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train Simple NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_bdt_train_tensor = torch.tensor(X_nn_bdt_train, dtype=torch.float32)\n",
    "X_xgb_train_tensor = torch.tensor(X_nn_xgb_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_nn_train, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_bdt_test_tensor = torch.tensor(X_nn_bdt_test, dtype=torch.float32)\n",
    "X_xgb_test_tensor = torch.tensor(X_nn_xgb_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_nn_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Create DataLoaders\n",
    "bdt_train_loader = DataLoader(TensorDataset(X_bdt_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "xgb_train_loader = DataLoader(TensorDataset(X_xgb_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "# Define a simple NN model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize models\n",
    "nn_bdt = SimpleNN(12)  # 12 BDT features\n",
    "nn_xgb = SimpleNN(12)  # 12 XGB features\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_bdt = optim.Adam(nn_bdt.parameters(), lr=0.001)\n",
    "optimizer_xgb = optim.Adam(nn_xgb.parameters(), lr=0.001)\n",
    "\n",
    "loss_history_bdt = []\n",
    "loss_history_xgb = []\n",
    "\n",
    "# Train NN_BDT and track loss\n",
    "print(\"Training NN_BDT...\")\n",
    "for epoch in range(30):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_X, batch_y in bdt_train_loader:\n",
    "        optimizer_bdt.zero_grad()\n",
    "        outputs = nn_bdt(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_bdt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    loss_history_bdt.append(avg_loss)\n",
    "    # print(f\"Epoch {epoch+1}/30, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(range(1, 31), loss_history_bdt, label=\"NN_BDT Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve for NN_BDT\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Train NN_XGB\n",
    "for epoch in range(30):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_X, batch_y in xgb_train_loader:\n",
    "        optimizer_xgb.zero_grad()\n",
    "        outputs = nn_xgb(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer_xgb.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    loss_history_xgb.append(avg_loss)\n",
    "    # print(f\"Epoch {epoch+1}/30, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(range(1, 31), loss_history_bdt, label=\"NN_XGB Training Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve for NN_XGB\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize NN Cutoff for Signal Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get NN Outputs on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "nn_bdt_output = nn_bdt(X_bdt_test_tensor).detach().numpy().flatten()\n",
    "nn_xgb_output = nn_xgb(X_xgb_test_tensor).detach().numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load background Type Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load background types for test set\n",
    "background_types_train = joblib.load('split_datasets/background_types_train.pkl')\n",
    "background_types_val = joblib.load('split_datasets/background_types_val.pkl')\n",
    "background_types_test = joblib.load('split_datasets/background_types_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Cutoff for Signal Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_significance(threshold, predictions, true_labels, background_types):\n",
    "    \"\"\"\n",
    "    Computes signal significance and returns signal count, background counts per type, and weighted significance.\n",
    "    \n",
    "    Parameters:\n",
    "        threshold (float): Cutoff threshold for classification.\n",
    "        predictions (array): Array of predicted scores.\n",
    "        true_labels (array): Ground truth labels (1 for signal, 0 for background).\n",
    "        background_types (list): List of background type strings corresponding to each background event.\n",
    "    \n",
    "    Returns:\n",
    "        significance (float): The computed signal significance.\n",
    "        signal_count (int): The number of signal events passing the threshold.\n",
    "        background_counts (dict): Dictionary mapping background type to its event count.\n",
    "    \"\"\"\n",
    "    # Initialize signal count and weight\n",
    "    signal_count = 0\n",
    "    signal_weight = background_weights.get(\"HH\", 1)  # Default to 1 if not found\n",
    "\n",
    "    # Initialize background counts per type\n",
    "    background_counts = {bg_type: 0 for bg_type in background_weights.keys()}\n",
    "    background_sum = 0  # Weighted sum of background counts\n",
    "\n",
    "    j = -1  # Background indexing\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] >= threshold:\n",
    "            if true_labels[i] == 1:\n",
    "                signal_count += 1  # Count signal event\n",
    "            else:\n",
    "                j += 1  # Increment background index\n",
    "                bg_type = background_types[j]  # Get background type\n",
    "                if bg_type in background_counts:\n",
    "                    background_counts[bg_type] += 1  # Count background event\n",
    "                    background_sum += background_weights.get(bg_type, 0)  # Add weighted count\n",
    "        else:\n",
    "            if true_labels[i] == 0:\n",
    "                j += 1\n",
    "\n",
    "    # Compute weighted signal\n",
    "    weighted_signal = signal_count * signal_weight\n",
    "\n",
    "    # Compute significance\n",
    "    if weighted_signal + background_sum > 0:\n",
    "        significance = weighted_signal / np.sqrt(weighted_signal + background_sum)\n",
    "    else:\n",
    "        significance = 0\n",
    "\n",
    "    return significance, signal_count, background_counts\n",
    "\n",
    "\n",
    "def compute_asimov_significance(threshold, predictions, true_labels):\n",
    "    S = np.sum((predictions >= threshold) & (true_labels == 1))\n",
    "    B = np.sum((predictions >= threshold) & (true_labels == 0))\n",
    "    \n",
    "    if B == 0:\n",
    "        return np.sqrt(2 * S)  # Asimov approximation when B = 0\n",
    "    \n",
    "    return np.sqrt(2 * (S + B * np.log(1 + S / B) - B))\n",
    "\n",
    "# Sweep through cutoffs\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "\n",
    "best_threshold_bdt, best_threshold_xgb = 0, 0\n",
    "best_significance_bdt, best_significance_xgb = 0, 0\n",
    "best_signal_count_bdt, best_background_count_bdt = 0, {bg_type: 0 for bg_type in background_weights.keys()}\n",
    "best_signal_count_xgb, best_background_count_xgb = 0, {bg_type: 0 for bg_type in background_weights.keys()}\n",
    "\n",
    "# best_asimov_threshold_bdt, best_asimov_threshold_xgb = 0, 0\n",
    "# best_asimov_significance_bdt, best_asimov_significance_xgb = 0, 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # BDT model\n",
    "    significance_bdt, signal_counts_bdt, background_counts_bdt = compute_significance(threshold, nn_bdt_output, y_nn_test, background_types_test)\n",
    "    # asimov_significance_bdt = compute_asimov_significance(threshold, nn_bdt_output, y_nn_test)\n",
    "    \n",
    "    if significance_bdt > best_significance_bdt:\n",
    "        best_significance_bdt = significance_bdt\n",
    "        best_threshold_bdt = threshold\n",
    "        best_signal_count_bdt = signal_counts_bdt\n",
    "        best_background_count_bdt = background_counts_bdt\n",
    "\n",
    "    # if asimov_significance_bdt > best_asimov_significance_bdt:\n",
    "    #     best_asimov_significance_bdt = asimov_significance_bdt\n",
    "    #     best_asimov_threshold_bdt = threshold\n",
    "\n",
    "    # XGB model\n",
    "    significance_xgb, signal_counts_xgb, background_counts_xgb = compute_significance(threshold, nn_xgb_output, y_nn_test, background_types_test)\n",
    "    # asimov_significance_xgb = compute_asimov_significance(threshold, nn_xgb_output, y_nn_test)\n",
    "    \n",
    "    if significance_xgb > best_significance_xgb:\n",
    "        best_significance_xgb = significance_xgb\n",
    "        best_threshold_xgb = threshold\n",
    "        best_signal_count_xgb = signal_counts_xgb\n",
    "        best_background_count_xgb = background_counts_xgb\n",
    "\n",
    "    # if asimov_significance_xgb > best_asimov_significance_xgb:\n",
    "    #     best_asimov_significance_xgb = asimov_significance_xgb\n",
    "    #     best_asimov_threshold_xgb = threshold\n",
    "\n",
    "print(f\"NN_BDT: Best Threshold = {best_threshold_bdt:.2f}, Max Significance = {best_significance_bdt:.2f}\")\n",
    "print(f\"NN_BDT: Signal Counts = {best_signal_count_bdt}, Background Counts = {best_background_count_bdt}\")\n",
    "print(f\"NN_XGB: Best Threshold = {best_threshold_xgb:.2f}, Max Significance = {best_significance_xgb:.2f}\")\n",
    "print(f\"NN_XGB: Signal Counts = {best_signal_count_xgb}, Background Counts = {best_background_count_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of NN outputs for signal and background (BDT)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(nn_bdt_output[y_nn_test == 1], bins=50, color='red', label=\"Signal\", kde=True)\n",
    "sns.histplot(nn_bdt_output[y_nn_test == 0], bins=50, color='blue', label=\"Background\", kde=True)\n",
    "\n",
    "# Add vertical line for best threshold\n",
    "plt.axvline(x=best_threshold_bdt, color='black', linestyle='--', linewidth=2, label=f\"Best Threshold: {best_threshold_bdt:.2f}\")\n",
    "\n",
    "plt.title(\"NN_BDT Output Probability Distribution\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of NN outputs for signal and background (XGB)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(nn_xgb_output[y_nn_test == 1], bins=50, color='red', label=\"Signal\", kde=True)\n",
    "sns.histplot(nn_xgb_output[y_nn_test == 0], bins=50, color='blue', label=\"Background\", kde=True)\n",
    "\n",
    "# Add vertical line for best threshold\n",
    "plt.axvline(x=best_threshold_xgb, color='black', linestyle='--', linewidth=2, label=f\"Best Threshold: {best_threshold_xgb:.2f}\")\n",
    "\n",
    "plt.title(\"NN_XGB Output Probability Distribution\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimization problem to maximize significance\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximizing significance\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "weightHH = background_weights.get(\"HH\")\n",
    "\n",
    "# Function to create an individual (12 cutoffs for 12 BDT/XGB models)\n",
    "def create_individual():\n",
    "    cutoffs = [random.uniform(0, 1) for _ in range(12)]\n",
    "    return creator.Individual(cutoffs)\n",
    "\n",
    "# Function to compute signal significance\n",
    "def compute_significance(thresholds, predictions, true_labels, background_types):\n",
    "    thresholded_preds = (predictions >= thresholds.reshape(1, -1)).astype(int)\n",
    "    final_preds = np.all(thresholded_preds, axis=1).astype(int)\n",
    "    signal_count = np.sum((final_preds == 1) & (true_labels == 1))\n",
    "    weighted_signal = signal_count * weightHH\n",
    "\n",
    "    # Background processing\n",
    "    background_sums = {bg: 0 for bg in background_weights}\n",
    "    background_idx = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if true_labels[i] == 0 and final_preds[i] == 1:  # Background sample passing threshold\n",
    "            bg_type = background_types[background_idx]\n",
    "            if bg_type in background_weights:\n",
    "                background_sums[bg_type] += background_weights[bg_type]\n",
    "            background_idx += 1\n",
    "        elif true_labels[i] == 0:\n",
    "            background_idx += 1\n",
    "\n",
    "    background_sum = sum(background_sums.values())\n",
    "\n",
    "    # Compute significance\n",
    "    if weighted_signal + background_sum > 0:\n",
    "        significance = weighted_signal / np.sqrt(weighted_signal + background_sum)\n",
    "    else:\n",
    "        significance = 0\n",
    "\n",
    "    return significance\n",
    "\n",
    "\n",
    "def evaluate(individual, predictions, labels, background_types):\n",
    "    thresholds = np.array(individual)  # Convert to NumPy array\n",
    "\n",
    "    # Ensure thresholds is a 1D array of shape (12,)\n",
    "    if thresholds.shape != (12,):  \n",
    "        print(f\"ðŸš¨ Error: thresholds has shape {thresholds.shape}, expected (12,)\")\n",
    "        thresholds = np.array([individual] * 12)  # Fix if needed\n",
    "\n",
    "    significance = compute_significance(thresholds, predictions, labels, background_types)\n",
    "    return (significance,)  # DEAP requires a tuple\n",
    "\n",
    "\n",
    "# Create GA toolbox\n",
    "def create_ga_toolbox(predictions, labels, background_types):\n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"individual\", create_individual)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"evaluate\", evaluate, predictions=predictions, labels=labels, background_types=background_types)\n",
    "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)\n",
    "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "    return toolbox\n",
    "\n",
    "# Train GA function\n",
    "def train_ga(toolbox, pop, ngen=300, cxpb=0.5, mutpb=0.2, conv_steps=20, conv_crit=0.001):\n",
    "    hof = tools.HallOfFame(1)\n",
    "    best_fitness = None\n",
    "    stagnation_count = 0\n",
    "    \n",
    "    for gen in range(ngen):\n",
    "        algorithms.eaSimple(pop, toolbox, cxpb, mutpb, 1, stats=None, halloffame=hof, verbose=False)\n",
    "        current_fitness = hof[0].fitness.values[0]\n",
    "        \n",
    "        # Print signal significance for this generation\n",
    "        print(f\"Generation {gen+1}: Significance = {current_fitness:.4f}, Stagnation = {stagnation_count}\")\n",
    "\n",
    "        if best_fitness is None or current_fitness > best_fitness + conv_crit:\n",
    "            best_fitness = current_fitness\n",
    "            stagnation_count = 0\n",
    "        else:\n",
    "            stagnation_count += 1\n",
    "        \n",
    "        if stagnation_count >= conv_steps:\n",
    "            print(\"Stopping early due to convergence.\")\n",
    "            break\n",
    "    return hof[0]\n",
    "\n",
    "# Create GA toolboxes for BDT and XGB\n",
    "toolbox_bdt = create_ga_toolbox(X_nn_bdt_train, y_nn_train, background_types_val)\n",
    "toolbox_xgb = create_ga_toolbox(X_nn_xgb_train, y_nn_train, background_types_val)\n",
    "\n",
    "# Initialize populations\n",
    "pop_bdt = toolbox_bdt.population(n=100)\n",
    "pop_xgb = toolbox_xgb.population(n=100)\n",
    "\n",
    "# Implement spread control\n",
    "def spread_control(toolbox, pop, best_fitness, scsteps, scrate, scfactor):\n",
    "    improvements = 0\n",
    "    for ind in pop:\n",
    "        if ind.fitness.values[0] > best_fitness:\n",
    "            improvements += 1\n",
    "    if improvements < scrate:\n",
    "        toolbox.unregister(\"mutate\")\n",
    "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1 / scfactor, indpb=0.2)\n",
    "    elif improvements > scrate:\n",
    "        toolbox.unregister(\"mutate\")\n",
    "        toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1 * scfactor, indpb=0.2)\n",
    "\n",
    "# Train GA function with dynamic mutation rate and spread control\n",
    "def train_ga(toolbox, pop, ngen=300, cxpb=0.5, mutpb=0.2, conv_steps=20, conv_crit=0.001, scsteps=10, scrate=5, scfactor=0.95):\n",
    "    hof = tools.HallOfFame(1)\n",
    "    best_fitness = None\n",
    "    stagnation_count = 0\n",
    "    \n",
    "    for gen in range(ngen):\n",
    "        if gen < 10:\n",
    "            toolbox.unregister(\"mutate\")\n",
    "            toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.2, indpb=0.2)\n",
    "        else:\n",
    "            toolbox.unregister(\"mutate\")\n",
    "            toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.1, indpb=0.2)\n",
    "        \n",
    "        algorithms.eaSimple(pop, toolbox, cxpb, mutpb, 1, stats=None, halloffame=hof, verbose=False)\n",
    "        current_fitness = hof[0].fitness.values[0]\n",
    "        \n",
    "        # Print signal significance for this generation\n",
    "        print(f\"Generation {gen+1}: Significance = {current_fitness:.4f}, Stagnation = {stagnation_count}\")\n",
    "\n",
    "        if best_fitness is None or current_fitness > best_fitness + conv_crit:\n",
    "            best_fitness = current_fitness\n",
    "            stagnation_count = 0\n",
    "        else:\n",
    "            stagnation_count += 1\n",
    "        \n",
    "        if stagnation_count >= conv_steps:\n",
    "            print(\"Stopping early due to convergence.\")\n",
    "            break\n",
    "        \n",
    "        spread_control(toolbox, pop, best_fitness, scsteps, scrate, scfactor)\n",
    "    \n",
    "    return hof[0]\n",
    "\n",
    "# Run GA training for BDT and XGB\n",
    "best_w_bdt = train_ga(toolbox_bdt, pop_bdt)\n",
    "best_w_xgb = train_ga(toolbox_xgb, pop_xgb)\n",
    "\n",
    "# Extract best weights (12 values each)\n",
    "w_bdt = np.array(best_w_bdt)\n",
    "w_xgb = np.array(best_w_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GA Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply optimized 12-dimensional weights to test set\n",
    "y_pred_bdt_final = np.all((X_nn_bdt_test >= w_bdt[None, :]), axis=1).astype(int)\n",
    "y_pred_xgb_final = np.all((X_nn_xgb_test >= w_xgb[None, :]), axis=1).astype(int)\n",
    "\n",
    "# Evaluate test performance\n",
    "significance_bdt = compute_significance(w_bdt, X_nn_bdt_test, y_nn_test, background_types_test)\n",
    "significance_xgb = compute_significance(w_xgb, X_nn_xgb_test, y_nn_test, background_types_test)\n",
    "\n",
    "print(f\"Optimized Cutoffs for BDT: {w_bdt}\")\n",
    "print(f\"Optimized Cutoffs for XGB: {w_xgb}\")\n",
    "print(f\"Test Significance (BDT): {significance_bdt:.4f}\")\n",
    "print(f\"Test Significance (XGB): {significance_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply optimized cutoffs to test set\n",
    "final_preds_bdt = np.all((X_nn_bdt_test >= w_bdt[None, :]), axis=1).astype(int)\n",
    "final_preds_xgb = np.all((X_nn_xgb_test >= w_xgb[None, :]), axis=1).astype(int)\n",
    "\n",
    "# Count signals\n",
    "signal_count_bdt = np.sum((final_preds_bdt == 1) & (y_nn_test == 1))\n",
    "signal_count_xgb = np.sum((final_preds_xgb == 1) & (y_nn_test == 1))\n",
    "\n",
    "# Initialize background counts\n",
    "background_counts_bdt = {bg: 0 for bg in background_weights}\n",
    "background_counts_xgb = {bg: 0 for bg in background_weights}\n",
    "\n",
    "# Background index counter\n",
    "background_idx = 0\n",
    "\n",
    "# Count backgrounds that passed for BDT\n",
    "for i in range(len(y_nn_test)):\n",
    "    if y_nn_test[i] == 0 and final_preds_bdt[i] == 1:  # If background and classified as signal\n",
    "        bg_type = background_types_test[background_idx]  # Get background type\n",
    "        if bg_type in background_counts_bdt:\n",
    "            background_counts_bdt[bg_type] += 1\n",
    "        background_idx += 1\n",
    "    elif y_nn_test[i] == 0:\n",
    "        background_idx += 1\n",
    "\n",
    "# Reset background index counter for XGB\n",
    "background_idx = 0\n",
    "\n",
    "# Count backgrounds that passed for XGB\n",
    "for i in range(len(y_nn_test)):\n",
    "    if y_nn_test[i] == 0 and final_preds_xgb[i] == 1:\n",
    "        bg_type = background_types_test[background_idx]\n",
    "        if bg_type in background_counts_xgb:\n",
    "            background_counts_xgb[bg_type] += 1\n",
    "        background_idx += 1\n",
    "    elif y_nn_test[i] == 0:\n",
    "        background_idx += 1\n",
    "\n",
    "# Print final counts\n",
    "print(f\"Signals Passing Cutoff (BDT): {signal_count_bdt}\")\n",
    "print(f\"Background Counts Passing Cutoff (BDT): {background_counts_bdt}\")\n",
    "print(f\"Signals Passing Cutoff (XGB): {signal_count_xgb}\")\n",
    "print(f\"Background Counts Passing Cutoff (XGB): {background_counts_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
